{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Datasets Introduction for TensorFlow v.1.3.](# https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html)  \n",
    "\n",
    "cf. Blogpost [Introduction to TensorFlow Datasets and Estimators\n",
    "Tuesday, September 12, 2017\n",
    "](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html), for the [Google Developers Blog](https://developers.googleblog.com/), https://goo.gl/Ujm2Ep  \n",
    "\n",
    "Also the code in Python was [here](https://github.com/mhyttsten/Misc/blob/master/Blog_Estimators_DataSet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Check that we have the correct TensorFlow version installed  \n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.3\" <= tf_version, \"TensorFlow r1.3 or later is needed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "# Windows users: You only need to change PATH, \n",
    "# otherwise I have changed this PATH to be a local relative path `./` \n",
    "# from the original, PATH = \"/tmp/tf_dataset_and_estimator_apis\"\n",
    "print(os.sep)\n",
    "PATH = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch and store Training and Test dataset files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset\n"
     ]
    }
   ],
   "source": [
    "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "print(PATH_DATASET)\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST  = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN  = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST   = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = urllib.urlopen(url).read()\n",
    "        with open(file,\"w\") as f:\n",
    "            f.write(data)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloadDataset(URL_TRAIN,FILE_TRAIN)\n",
    "downloadDataset(URL_TEST,FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_training.csv', 'iris_test.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir( PATH_DATASET )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.logging.set_verbosity(tf.logging.INFO)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Datasets   \n",
    "\n",
    "cf. https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html  \n",
    "\n",
    "Datasets is a new way to create input pipelines to TensorFlow models.  This API is much more performant than using `feed_dict` or the queue-based pipelines, and it's cleaner and easier to use.  \n",
    "\n",
    "Although Datasets still resides in `tf.contrib.data` at 1.3, it's expected this API will move to core at 1.4, so it's high time to take it for a test drive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high-level, Datasets consists of the following classes and relations:\n",
    "\n",
    "$$\n",
    "\\text{TextLineDataset}, \\text{TFRecordDataset}, \\text{FixedLengthRecordDataset} \\in \\text{subclass} \\to \\text{Dataset} \\xrightarrow{\\text{instantiates}} \\text{Iterator}\n",
    "$$  \n",
    "\n",
    "where  \n",
    "* Dataset : Base class containing methods to create and transform datasets.  Also, allows you to initialize a dataset from data in memory, or from a Python generator  \n",
    "* TextLineDataset : Reads lines from text files.  \n",
    "* TFRecordDataset : Reads records from TFRecord files.\n",
    "* FixedLengthRecordDataset : Reads fixed size records from binary files. \n",
    "* Iterator : Provdes a way to access 1 dataset element at a time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an input function, reading a file using the Dataset API.  \n",
    "Then provide the results to the Estimator API.  \n",
    "\n",
    "Arguments for this function are:\n",
    "* `file_path` : data file to read\n",
    "* `perform_shuffle` : whether record order should be randomized \n",
    "* `repeat_count` : number of times to iterate over records in the dataset.  e.g. if `repeat_count=1`, then each record is read once.  If `repeat_count=None`, iteration will continue forever.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.],[0.,],[0.,],[0.,], [0]])\n",
    "        label = parsed_line[-1:] # Last element is the label\n",
    "        del parsed_line[-1] # Delete last element\n",
    "        features = parsed_line # Everything but last elements are the features \n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d  \n",
    "    \n",
    "    dataset = (tf.contrib.data.TextLineDataset(file_path) # Read text file \n",
    "              .skip(1) # Skip header row\n",
    "              .map(decode_csv)) # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count) # Repeats dataset this # times \n",
    "    dataset = dataset.batch(32) # Batch size to use \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels= iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our model, we'll need a function that reads the input file and returns the feature and label data.  Estimators require that you create a function that'll have a return value of a tuple.  \n",
    "\n",
    "The return value must be a 2-element tuple organized as follows:\n",
    "* The 1st element must be a `dict` in which each input feature is a key, and then a list of values for the training batch.  \n",
    "* 2nd element is a list of labels for the training batch.  \n",
    "\n",
    "Since we're returning a batch of input features and training labels, it means that all lists in the return statement will have equal lengths.  Technically speaking, whenever we referred to \"list\" here, we actually mean a 1-dim. TensorFlow tensor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following: \n",
    "* `TextLineDataset` : The Dataset API will do a lot of memory management for you when you're using its file-based datasets.  You can, for example, read in dataset files much larger than memory or read in multiple files by specifying a list as argument \n",
    "* `shuffle` : reads buffer_size records, then shuffles (randomizes) their order \n",
    "* `map` : calls the `decode_csv` function with each element in the dataset as an argument (since we're using `TextLineDataset`, each element will be a line of `.csv` text).  Then we apply `decode_csv` to each of the lines  \n",
    "* `decode_csv` : splits each line into fields, providing the default values if necessary.  Then returns a dict with the field keys and field values.  The map function updates each elem (line) in the dataset with the `dict`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an introduction to Datasets!  For fun, use this function to print the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_batch = my_input_fn(FILE_TRAIN, True) # Will return 32 random elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n",
      "2\n",
      "<type 'dict'>\n",
      "['SepalLength', 'PetalWidth', 'PetalLength', 'SepalWidth']\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(32,)\n",
      "(32,)\n",
      "(32,)\n",
      "(32,)\n",
      "<type 'numpy.ndarray'>\n",
      "(32, 1)\n",
      "({'SepalLength': array([ 5.        ,  4.4000001 ,  5.        ,  6.69999981,  5.        ,\n",
      "        5.        ,  4.69999981,  6.5       ,  5.        ,  6.4000001 ,\n",
      "        7.69999981,  6.30000019,  4.4000001 ,  4.80000019,  5.69999981,\n",
      "        6.        ,  6.4000001 ,  5.9000001 ,  6.        ,  5.        ,\n",
      "        5.19999981,  6.        ,  6.0999999 ,  5.0999999 ,  6.0999999 ,\n",
      "        5.        ,  7.30000019,  5.69999981,  5.5999999 ,  4.9000001 ,\n",
      "        4.4000001 ,  6.5       ], dtype=float32), 'PetalWidth': array([ 1.        ,  0.2       ,  0.2       ,  2.29999995,  0.40000001,\n",
      "        0.60000002,  0.2       ,  2.20000005,  0.30000001,  1.79999995,\n",
      "        2.20000005,  2.4000001 ,  0.2       ,  0.1       ,  1.29999995,\n",
      "        1.60000002,  1.89999998,  1.79999995,  1.5       ,  0.2       ,\n",
      "        0.2       ,  1.5       ,  1.29999995,  0.40000001,  1.39999998,\n",
      "        1.        ,  1.79999995,  1.29999995,  1.10000002,  1.70000005,\n",
      "        0.2       ,  2.        ], dtype=float32), 'PetalLength': array([ 3.29999995,  1.39999998,  1.20000005,  5.19999981,  1.60000002,\n",
      "        1.60000002,  1.29999995,  5.80000019,  1.29999995,  5.5       ,\n",
      "        6.69999981,  5.5999999 ,  1.29999995,  1.39999998,  4.19999981,\n",
      "        5.0999999 ,  5.30000019,  5.0999999 ,  4.5       ,  1.5       ,\n",
      "        1.39999998,  5.        ,  4.        ,  1.5       ,  4.69999981,\n",
      "        3.5       ,  6.30000019,  4.0999999 ,  3.9000001 ,  4.5       ,\n",
      "        1.29999995,  5.0999999 ], dtype=float32), 'SepalWidth': array([ 2.29999995,  2.9000001 ,  3.20000005,  3.        ,  3.4000001 ,\n",
      "        3.5       ,  3.20000005,  3.        ,  3.5       ,  3.0999999 ,\n",
      "        3.79999995,  3.4000001 ,  3.        ,  3.        ,  2.9000001 ,\n",
      "        2.70000005,  2.70000005,  3.        ,  2.9000001 ,  3.4000001 ,\n",
      "        3.4000001 ,  2.20000005,  2.79999995,  3.70000005,  2.9000001 ,\n",
      "        2.        ,  2.9000001 ,  2.79999995,  2.5       ,  2.5       ,\n",
      "        3.20000005,  3.20000005], dtype=float32)}, array([[1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [2],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [2],\n",
      "       [0],\n",
      "       [2],\n",
      "       [2],\n",
      "       [2],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [2],\n",
      "       [2],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [2],\n",
      "       [1],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [2],\n",
      "       [1],\n",
      "       [1],\n",
      "       [2],\n",
      "       [0],\n",
      "       [2]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "# Now let's try it out, retrieving and printing 1 batch of data.\n",
    "# Although this code looks strange, you don't need to understand\n",
    "# the details\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "print(type(first_batch)) # tuple\n",
    "print(len(first_batch)) # 2\n",
    "print(type(first_batch[0])) # dict\n",
    "print(first_batch[0].keys())\n",
    "print(type(first_batch[0]['SepalLength'] )) # np.array\n",
    "print(type(first_batch[0][ feature_names[1]] )) # np.array\n",
    "print(type(first_batch[0][ feature_names[2]] )) # np.array\n",
    "print(type(first_batch[0][ feature_names[3]] )) # np.array\n",
    "print(first_batch[0]['SepalLength'].shape ) # np.array\n",
    "print(first_batch[0][ feature_names[1]].shape ) # np.array\n",
    "print(first_batch[0][ feature_names[2]].shape ) # np.array\n",
    "print(first_batch[0][ feature_names[3]].shape ) # np.array\n",
    "print(type(first_batch[1])) # np.array\n",
    "print(first_batch[1].shape) # (32,1)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, as a sanity check, we can look at the `.csv` files in `pandas`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   120    4  setosa  versicolor  virginica\n",
      "0  6.4  2.8     5.6         2.2          2\n",
      "1  5.0  2.3     3.3         1.0          1\n",
      "2  4.9  2.5     4.5         1.7          2\n",
      "3  4.9  3.1     1.5         0.1          0\n",
      "4  5.7  3.8     1.7         0.3          0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>120</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.845000</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>3.739167</td>\n",
       "      <td>1.196667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.868578</td>\n",
       "      <td>0.427156</td>\n",
       "      <td>1.822100</td>\n",
       "      <td>0.782039</td>\n",
       "      <td>0.840168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.075000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.425000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              120           4      setosa  versicolor   virginica\n",
       "count  120.000000  120.000000  120.000000  120.000000  120.000000\n",
       "mean     5.845000    3.065000    3.739167    1.196667    1.000000\n",
       "std      0.868578    0.427156    1.822100    0.782039    0.840168\n",
       "min      4.400000    2.000000    1.000000    0.100000    0.000000\n",
       "25%      5.075000    2.800000    1.500000    0.300000    0.000000\n",
       "50%      5.800000    3.000000    4.400000    1.300000    1.000000\n",
       "75%      6.425000    3.300000    5.100000    1.800000    2.000000\n",
       "max      7.900000    4.400000    6.900000    2.500000    2.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_DF = pd.read_csv(FILE_TRAIN)\n",
    "print(first_batch_DF.head() )\n",
    "first_batch_DF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Estimators  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one \n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_model_dir': '.', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create a deep neural network regression classifier \n",
    "# Use the DNNClassifier pre-made estimator  \n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns, # The input features to our model\n",
    "    hidden_units=[10,10], # Two layers, each with 10 neurons\n",
    "    n_classes=3,\n",
    "    model_dir=PATH) # Path to where checkpoints etc. are stored  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt-30\n",
      "INFO:tensorflow:Saving checkpoints for 31 into ./model.ckpt.\n",
      "INFO:tensorflow:loss = 10.5335, step = 31\n",
      "INFO:tensorflow:Saving checkpoints for 60 into ./model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.40039.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f86528fff90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our model, use the previous function my_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of the training of the data (epochs)\n",
    "classifier.train(\n",
    "    input_fn=lambda: my_input_fn(FILE_TRAIN,True,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lambda: my_input_fn(FILE_TRAIN, True, 8)` is where we hook up Datasets with the Estimators!  \n",
    "\n",
    "Estimators need data to perform training, evaluation, and prediction, and it uses the `input_fn` to fetch the data.  \n",
    "\n",
    "Estimators require an `input_fn` with no arguments, so we create a function with no arguments using `lambda`, which calls `input_fn` with the desired arguments: `file_path`, `shuffle_setting`, `repeat_count`  \n",
    "\n",
    "In our case, we use our `my_input_fn`, passing it  \n",
    "* `FILE_TRAIN`, which is the training data file.  \n",
    "* `True`, which tells the Estimator to shuffle the data.\n",
    "* `8`, which tells the Estimator to and repeat the dataset 8 times.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Trained Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we evaluate how well it's performing?  Fortunately, every Estimator contains an `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-09-27-14:19:45\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt-60\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-27-14:19:45\n",
      "INFO:tensorflow:Saving dict for global step 60: accuracy = 0.866667, average_loss = 0.316627, global_step = 60, loss = 9.4988\n",
      "Evaluation results\n",
      "   average_loss, was: 0.316626608372\n",
      "   accuracy, was: 0.866666674614\n",
      "   global_step, was: 60\n",
      "   loss, was: 9.49879837036\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as : loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 4))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Making Predictions Using Our Trained Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test file\n"
     ]
    }
   ],
   "source": [
    "# Predict the type of some Iris flowers. \n",
    "# Let's predict the examples in FILE_TEST, repeat only once.  \n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST,False,1))\n",
    "print(\"Predictions on test file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt-60\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e.: 0, ,1, or 2 if the prediction \n",
    "    # is Iris Sentosa, Vericolor, Virginica, respectively.  \n",
    "    print(prediction[\"class_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions on Data in Memory  \n",
    "\n",
    "How could we make predictions on data residing in other sources, for example, in memory?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let create a memory dataset for prediction.  \n",
    "# We've taken the first 3 examples in FILE_TEST.  \n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5], # -> 1, Iris Versicolor \n",
    "                    [6.9, 3.1, 5.4, 2.1], # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]] # -> 0, Iris Sentosa \n",
    "\n",
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4) # Need to split into our 4 features\n",
    "        # When predicting, we don't need (or have) any labels\n",
    "        return dict(zip(feature_names, x)) # To build a dict of them\n",
    "    \n",
    "    # The from_tensor_slices function will use a memory structure as input\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None # In prediction, we have no labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on memory data\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt-60\n",
      "I think: [5.9, 3.0, 4.2, 1.5], is Iris Versicolor\n",
      "I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Predictions on memory data\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0] # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " ' '"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
